{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_datareader.data as web\n",
    "from datetime import datetime, date\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM,Dropout\n",
    "import keras\n",
    "import h5py as h5py\n",
    "import cython\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('D:\\Stanford\\CS230\\Project\\Final')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = pd.read_csv('historical_stock_prices.csv')\n",
    "stock_data['date'] = pd.to_datetime(stock_data.date)\n",
    "start_date = '2003-01-01'\n",
    "end_date = '2013-12-31'\n",
    "stock_data0313 = stock_data.loc[(stock_data['date'] > start_date) & (stock_data['date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_stats = pd.read_csv('key_stats_yahoo.csv')\n",
    "key_stats.Ticker = key_stats['Ticker'].str.upper()\n",
    "key_stats.Date = pd.to_datetime(key_stats.Date)\n",
    "key_stats.Date = key_stats.Date.map(lambda x: x.strftime('%Y-%m-%d'))\n",
    "key_stats.Date = pd.to_datetime(key_stats.Date)\n",
    "key_stats = key_stats.rename(columns = {'Date':'date', 'Ticker':'ticker'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by market cap, and output the first 300 stocks\n",
    "key_stats_grouped = key_stats.groupby(by='ticker').sum()\n",
    "market_cap_ranked = key_stats_grouped['Market Cap']\n",
    "market_cap_ranked['BAC'] = 0\n",
    "market_cap_ranked = market_cap_ranked.sort_values(ascending=False)\n",
    "market_cap_ranked_names = market_cap_ranked.to_frame()\n",
    "market_cap_ranked_names = market_cap_ranked_names.reset_index()['ticker']\n",
    "\n",
    "num_stocks = 10\n",
    "\n",
    "top_stocks = market_cap_ranked_names[:num_stocks]\n",
    "top_stocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the key statistic into the prices\n",
    "Price_stats0313 = pd.merge(stock_data0313, key_stats, how = 'left', on=['ticker','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_template = Price_stats0313[Price_stats0313.ticker == 'GOOG']\n",
    "return_template = return_template.loc[(return_template['date'] >= '2013-01-01') & (return_template['date'] <= '2013-12-31')]\n",
    "return_template = return_template[['date']]\n",
    "return_template = return_template.reset_index()\n",
    "return_template_predicted = return_template.drop(columns=['index'])\n",
    "lo_return = return_template_predicted\n",
    "ls_return = return_template_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of days to use for the prediction\n",
    "window_size = 20\n",
    "\n",
    "train_loss_trend = {}\n",
    "dev_loss_trend = {}\n",
    "train_loss = {}\n",
    "dev_loss = {}\n",
    "test_mse = {}\n",
    "hit_ratio_list={}\n",
    "test_mae={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_data_structure(data, window_size):\n",
    "    y = data[window_size:]['adj_close']\n",
    "    n = data.shape[0]\n",
    "    X = np.stack([data[i: j] for i, j in enumerate(range(window_size, n))], axis=0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(top_stocks)):\n",
    "\n",
    "    #globals()[top_stocks[0] + '_data'] = Price_stats0313[Price_stats0313.ticker == top_stocks[0]]\n",
    "    selected_data = Price_stats0313[Price_stats0313.ticker == top_stocks[i]]\n",
    "    selected_data = selected_data.sort_values('date')\n",
    "\n",
    "    #use forward fill to deal with missing data\n",
    "    selected_data = selected_data.ffill(axis = 0)\n",
    "\n",
    "    selected_data = selected_data.sort_values('date', ascending = False)\n",
    "    selected_data = selected_data.ffill(axis = 0) \n",
    "\n",
    "    selected_data = selected_data.sort_values('date')\n",
    "\n",
    "    selected_data_truncated = selected_data[selected_data['date'] > '2004-01-01']\n",
    "\n",
    "    selected_data_truncated = selected_data_truncated.drop(columns=['Price','Unnamed: 0','ticker'])\n",
    "    '''\n",
    "\n",
    "    Model 1 Does not work\n",
    "    selected_data_truncated = selected_data_truncated[['adj_close','date','volume','DE Ratio','Return on Equity',\n",
    "                                                                'Price/Book','Profit Margin','Diluted EPS','Beta','Trailing P/E','Price/Sales'\n",
    "                                                      ,'Return on Equity','PEG Ratio','Earnings Growth','Total Cash Per Share',\n",
    "                                                      'Current Ratio','Book Value Per Share','Enterprise Value/EBITDA']]\n",
    "\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    #Model 4 Works better\n",
    "    selected_data_truncated = selected_data_truncated[['adj_close','date','volume','DE Ratio','Return on Equity',\n",
    "                                                                'Price/Book','Profit Margin','Diluted EPS']]\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    #Model 2  \n",
    "    selected_data_truncated = selected_data_truncated[['adj_close','date','volume','DE Ratio','Return on Equity',\n",
    "                                                                'Price/Book','Profit Margin','Diluted EPS','Beta']]\n",
    "\n",
    "    '''\n",
    "    #Model 3\n",
    "    selected_data_truncated = selected_data_truncated[['adj_close','date','volume','DE Ratio','Return on Equity',\n",
    "                                                                'Price/Book','Profit Margin','Diluted EPS','Beta',\n",
    "                                                      'Return on Equity','PEG Ratio','Earnings Growth']]\n",
    "    '''\n",
    "\n",
    "\n",
    "    #selected_data_truncated.info()\n",
    "\n",
    "\n",
    "    selected_data_truncated.set_index('date', inplace=True)\n",
    "    #Apply min_max Scale\n",
    "    selected_data_truncated_scaled = selected_data_truncated.apply(minmax_scale)\n",
    "\n",
    "\n",
    "    #transform the data to fit the rnn\n",
    "    X, y = create_rnn_data_structure(selected_data_truncated_scaled, window_size=window_size)\n",
    "\n",
    "\n",
    "    #split the data into train,dev and test\n",
    "    selected_data_truncated_scaled_reset_index = selected_data_truncated_scaled.reset_index()\n",
    "\n",
    "    #We use 2012 data for development\n",
    "    dev_size = selected_data_truncated_scaled_reset_index[(selected_data_truncated_scaled_reset_index['date'] > '2012-01-01') & (selected_data_truncated_scaled_reset_index['date'] <= '2012-12-31')].date.nunique()\n",
    "\n",
    "    #we use 2013 data for testing\n",
    "    test_size = selected_data_truncated_scaled_reset_index[(selected_data_truncated_scaled_reset_index['date'] > '2013-01-01') & (selected_data_truncated_scaled_reset_index['date'] <= '2013-12-31')].date.nunique()\n",
    "\n",
    "    train_size = X.shape[0] - dev_size - test_size\n",
    "\n",
    "    X_train, y_train = X[:train_size], y[:train_size]\n",
    "    X_dev, y_dev = X[train_size:train_size + dev_size], y[train_size:train_size + dev_size]\n",
    "    X_test, y_test = X[train_size + dev_size:train_size + dev_size + test_size], y[train_size + dev_size:train_size + dev_size+test_size]\n",
    "\n",
    "    #X_train.shape, y_train.shape, X_dev.shape, y_dev.shape, X_test.shape, y_test.shape\n",
    "\n",
    "\n",
    "    # design network as 2 LSTM with correposning dropout layer, together with 1 dense layer and one output layer\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='tanh', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]), name = 'LSTM1'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(30, activation='tanh',name = 'LSTM2'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, name = 'FC1',activation='relu'))\n",
    "    model.add(Dense(1, name = 'Output',activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "\n",
    "    # fit network\n",
    "    stock_Model = model.fit(X_train, y_train, epochs=100, batch_size=30, validation_data=(X_dev, y_dev), verbose=2, shuffle=False)\n",
    "\n",
    "\n",
    "    #Store the training and development total losses\n",
    "    train_loss_trend[str(top_stocks[i])] = stock_Model.history['loss']\n",
    "    dev_loss_trend[str(top_stocks[i])] = stock_Model.history['val_loss']\n",
    "    train_loss[str(top_stocks[i])] = stock_Model.history['loss'][-1]\n",
    "    dev_loss[str(top_stocks[i])] = stock_Model.history['val_loss'][-1]\n",
    "\n",
    "    #Predict the future Price\n",
    "    y_pred = pd.DataFrame(model.predict(X_test), index=y_test.index)\n",
    "\n",
    "    #compute the mean squred error and mean squred error of the prediction\n",
    "    test_mse_stats = mean_squared_error(y_pred, y_test)\n",
    "\n",
    "    #Store the mean squred error for the testing stage\n",
    "    test_mse[str(top_stocks[i])] = test_mse_stats\n",
    "    \n",
    "    \n",
    "    #compute the mean absolute error and mean squred error of the prediction\n",
    "    test_mae_stats = mean_absolute_error(y_pred, y_test)\n",
    "    test_mae[str(top_stocks[i])] = test_mae_stats\n",
    "    \n",
    "\n",
    "    #Use the predicted value for bulding up the trading strategy\n",
    "    daily_predicted_return = y_pred/y_pred.shift(1)-1\n",
    "\n",
    "\n",
    "\n",
    "    #Compute the hit ratio\n",
    "    true_hit = ((y_test/y_test.shift(1)-1)>0)*1\n",
    "    predicted_hit = (daily_predicted_return > 0)*1\n",
    "    true_array = np.asarray(true_hit)\n",
    "    predicted_array = np.asarray(predicted_hit)\n",
    "    true_array = np.reshape(true_array, (len(true_array), 1))\n",
    "    hit_ratio = (true_array==predicted_array).sum()/len(predicted_hit)\n",
    "\n",
    "    #Store the hit ratio\n",
    "    hit_ratio_list[str(top_stocks[i])] = hit_ratio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Obtain the trading signal for long only strategy\n",
    "    signal = (daily_predicted_return > 0)*1\n",
    "    signal = signal.rename(columns = {0:'lo'})\n",
    "\n",
    "    #Obtain the trading signal for long/short strategy\n",
    "    variable_name = {1 : 1 , 0 : -1 }\n",
    "    signal['ls'] = signal['lo'].map(variable_name)\n",
    "\n",
    "\n",
    "    #extract the actual stock prices\n",
    "    actual_prices = stock_data0313[stock_data0313.ticker == top_stocks[i]]\n",
    "\n",
    "    #Get the first date and last date in the signal to cut the actual price in the correct range\n",
    "    first_day_signal = signal.index[0]\n",
    "    last_day_signal = signal.index[-1]\n",
    "    actual_prices13 = actual_prices.loc[(actual_prices['date'] >= first_day_signal) & (actual_prices['date'] <= last_day_signal)]\n",
    "\n",
    "    #compute the daily actual return, assuming we only trade intraday\n",
    "    actual_prices13['daily_actual_return'] = actual_prices13.close/actual_prices13.open - 1\n",
    "\n",
    "\n",
    "    #merge the signal and the return for computing the daily trading P&L\n",
    "    signal_merge = signal.reset_index()\n",
    "    signal_actual_return = pd.merge(signal_merge, actual_prices13, how='left', on='date')\n",
    "\n",
    "\n",
    "    #Compute the long only return\n",
    "    signal_actual_return['daily_lo_return_'+str(top_stocks[i])] = signal_actual_return.lo*signal_actual_return.daily_actual_return\n",
    "    #Compute the long/short return\n",
    "    signal_actual_return['daily_ls_return_'+str(top_stocks[i])] = signal_actual_return.ls*signal_actual_return.daily_actual_return\n",
    "    #Keep only the useful columns\n",
    "    signal_actual_return_truncated = signal_actual_return[['date', 'daily_lo_return_'+str(top_stocks[i]), 'daily_ls_return_'+str(top_stocks[i])]]\n",
    "\n",
    "    #Merge the only only return into our dataset\n",
    "    lo_return = pd.merge(lo_return, signal_actual_return_truncated, how = 'left', on='date')\n",
    "    lo_return = lo_return.drop(columns='daily_ls_return_'+str(top_stocks[i]))\n",
    "\n",
    "\n",
    "    #merge the short only return into our dataset\n",
    "    ls_return = pd.merge(ls_return, signal_actual_return_truncated, how = 'left', on='date')\n",
    "    ls_return = ls_return.drop(columns='daily_lo_return_'+str(top_stocks[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the portfolio Return\n",
    "lo_return.set_index('date', inplace=True)\n",
    "ls_return.set_index('date', inplace=True)\n",
    "daily_lo_portfolio_return = lo_return.sum(axis=1)\n",
    "daily_ls_portfolio_return = ls_return.sum(axis=1)\n",
    "\n",
    "#Compute the portfolio Cummulative Return\n",
    "lo_cum_return = (1 + daily_lo_portfolio_return).cumprod()\n",
    "ls_cum_return = (1 + daily_ls_portfolio_return).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output mse to csv\n",
    "(pd.DataFrame.from_dict(data=test_mse, orient='index')\n",
    "   .to_csv('model3_mse.csv', header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Save the return data\n",
    "lo_return.to_excel('D:\\Stanford\\CS230\\Project\\output\\loLSTreturnM3.xlsx', index = None, header=True)\n",
    "ls_return.to_excel('D:\\Stanford\\CS230\\Project\\output\\lsLSTreturnM3.xlsx', index = None, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save hit ratio\n",
    "(pd.DataFrame.from_dict(data=hit_ratio_list, orient='index')\n",
    "   .to_csv('model3_hitratio.csv', header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save mae\n",
    "(pd.DataFrame.from_dict(data=test_mae, orient='index')\n",
    "   .to_csv('model3_mae.csv', header=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
